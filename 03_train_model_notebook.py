# %% [markdown]
# # ğŸ“ U-Net ëª¨ë¸ í•™ìŠµ
#
# 256Ã—256 íƒ€ì¼ë¡œ Forward EM Near-Field Intensity Prediction U-Net ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
#
# **Output**: EM Near-Field Intensity (|Ex|Â² + |Ey|Â² + |Ez|Â²)
#
# ## ğŸ“‹ ëª©ì°¨
# 1. í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸
# 2. íŒŒë¼ë¯¸í„° ì„¤ì •
# 3. ë°ì´í„° ë¡œë” ìƒì„±
# 4. ëª¨ë¸ ìƒì„±
# 5. í•™ìŠµ
# 6. ê²°ê³¼ ì‹œê°í™”

# %% [markdown]
# ## 1. í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸

# %%
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# PyTorch ì½”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append('pytorch_codes')

from models import ForwardPhaseUNet, MultiScalePhaseUNet, PhaseAmplitudeUNet
from datasets import ForwardPhaseDataset, create_dataloaders
from utils import WeightedMSELoss, Trainer

# GPU í™•ì¸
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"âœ… PyTorch ì„¤ì • ì™„ë£Œ!")
print(f"   Device: {device}")
print(f"   PyTorch ë²„ì „: {torch.__version__}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

# %% [markdown]
# ## 2. íŒŒë¼ë¯¸í„° ì„¤ì •

# %%
# ==================== ë°ì´í„° íŒŒë¼ë¯¸í„° ====================
DATA_PATH = 'data/forward_intensity_tiles'
BATCH_SIZE = 16                    # íƒ€ì¼ ê¸°ë°˜ì´ë¯€ë¡œ ë” í° ë°°ì¹˜ ì‚¬ìš© ê°€ëŠ¥
NUM_WORKERS = 4                     # ë°ì´í„° ë¡œë”© ì›Œì»¤

# ==================== ëª¨ë¸ íŒŒë¼ë¯¸í„° ====================
MODEL_TYPE = 'basic'                # 'basic', 'multiscale', 'phase_amplitude'
LAYER_NUM = 5                       # U-Net ë ˆì´ì–´ ìˆ˜
BASE_FEATURES = 64                  # ê¸°ë³¸ feature ìˆ˜
DROPOUT_RATE = 0.2                  # Dropout ë¹„ìœ¨
USE_BATCHNORM = True                # BatchNorm ì‚¬ìš© ì—¬ë¶€

# ==================== í•™ìŠµ íŒŒë¼ë¯¸í„° ====================
NUM_EPOCHS = 100
LEARNING_RATE = 1e-4
LOSS_TYPE = 'mse'                   # 'mse', 'weighted_mse'

# ==================== ì²´í¬í¬ì¸íŠ¸ íŒŒë¼ë¯¸í„° ====================
CHECKPOINT_DIR = 'checkpoints'
LOG_DIR = 'logs'
EXPERIMENT_NAME = f'forward_phase_{MODEL_TYPE}_tiles'
SAVE_FREQ = 5                       # N epochë§ˆë‹¤ ì €ì¥
VISUALIZE_FREQ = 10                 # N epochë§ˆë‹¤ ì˜ˆì¸¡ ì‹œê°í™”

# ë””ë ‰í† ë¦¬ ìƒì„±
Path(CHECKPOINT_DIR).mkdir(exist_ok=True)
Path(LOG_DIR).mkdir(exist_ok=True)

print("âœ… íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!")
print(f"\nğŸ“Š í•™ìŠµ ì„¤ì •:")
print(f"   ë°ì´í„° ê²½ë¡œ: {DATA_PATH}")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   ëª¨ë¸ íƒ€ì…: {MODEL_TYPE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {SAVE_FREQ} epochë§ˆë‹¤")
print(f"   ì˜ˆì¸¡ ì‹œê°í™”: {VISUALIZE_FREQ} epochë§ˆë‹¤")
print(f"   Device: {device}")

# %% [markdown]
# ## 3. ë°ì´í„° ë¡œë” ìƒì„±

# %%
print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

# ì§ì ‘ train/val ë°ì´í„°ì…‹ ìƒì„± (02ë²ˆì—ì„œ ì´ë¯¸ ë‚˜ëˆˆ ê²ƒ ì‚¬ìš©)
from torch.utils.data import DataLoader

train_dataset = ForwardPhaseDataset(
    data_path=f'{DATA_PATH}/train',
    normalize=False
)

val_dataset = ForwardPhaseDataset(
    data_path=f'{DATA_PATH}/val',
    normalize=False
)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS
)

print("\nâœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!")
print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset)} ({len(train_loader)} ë°°ì¹˜)")
print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)} ({len(val_loader)} ë°°ì¹˜)")

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
sample = next(iter(train_loader))
print(f"\nğŸ“Š ë°°ì¹˜ í¬ê¸°:")
print(f"   ì…ë ¥: {sample['image'].shape}  # (batch, C, H, W)")
print(f"   ì¶œë ¥: {sample['target'].shape}")
print(f"   ì…ë ¥ ë²”ìœ„: [{sample['image'].min():.2f}, {sample['image'].max():.2f}]")
print(f"   ì¶œë ¥ ë²”ìœ„: [{sample['target'].min():.2f}, {sample['target'].max():.2f}]")

# %% [markdown]
# ## 4. ëª¨ë¸ ìƒì„±

# %%
print("ğŸ”¨ ëª¨ë¸ ìƒì„± ì¤‘...")

# ëª¨ë¸ ìƒì„±
if MODEL_TYPE == 'basic':
    model = ForwardPhaseUNet(
        in_channels=1,
        out_channels=1,
        layer_num=LAYER_NUM,
        base_features=BASE_FEATURES,
        dropout_rate=DROPOUT_RATE,
        output_activation='linear',
        use_batchnorm=USE_BATCHNORM
    )
elif MODEL_TYPE == 'multiscale':
    model = MultiScalePhaseUNet(
        in_channels=1,
        out_channels=1,
        layer_num=LAYER_NUM,
        base_features=BASE_FEATURES,
        dropout_rate=DROPOUT_RATE,
        use_batchnorm=USE_BATCHNORM
    )
elif MODEL_TYPE == 'phase_amplitude':
    model = PhaseAmplitudeUNet(
        in_channels=1,
        layer_num=LAYER_NUM,
        base_features=BASE_FEATURES,
        dropout_rate=DROPOUT_RATE,
        use_batchnorm=USE_BATCHNORM
    )

model = model.to(device)

# ëª¨ë¸ ì •ë³´ ì¶œë ¥
print("\nâœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
model.get_model_summary()

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜:")
print(f"   ì „ì²´: {total_params:,}")
print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")

# %% [markdown]
# ## 5. Loss Function ë° Optimizer ì„¤ì •

# %%
# Loss function
if LOSS_TYPE == 'mse':
    criterion = nn.MSELoss()
elif LOSS_TYPE == 'weighted_mse':
    criterion = WeightedMSELoss()

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10, verbose=True
)

print("âœ… Loss function ë° Optimizer ì„¤ì • ì™„ë£Œ!")
print(f"   Loss: {LOSS_TYPE}")
print(f"   Optimizer: Adam")
print(f"   Learning rate: {LEARNING_RATE}")

# %% [markdown]
# ## 6. í•™ìŠµ

# %%
print("\n" + "="*80)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*80)

# Trainer ìƒì„±
trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    checkpoint_dir=CHECKPOINT_DIR,
    log_dir=LOG_DIR,
    experiment_name=EXPERIMENT_NAME,
    visualize_freq=VISUALIZE_FREQ
)

# í•™ìŠµ ì‹¤í–‰
trainer.train(
    num_epochs=NUM_EPOCHS,
    save_freq=SAVE_FREQ
)

# í•™ìŠµ íˆìŠ¤í† ë¦¬ êµ¬ì„±
history = {
    'train_loss': trainer.train_losses,
    'val_loss': trainer.val_losses,
    'train_mse': trainer.train_mse,
    'val_mse': trainer.val_mse,
    'train_psnr': trainer.train_psnr,
    'val_psnr': trainer.val_psnr,
    'learning_rate': [optimizer.param_groups[0]['lr']] * NUM_EPOCHS  # ê°„ë‹¨í•œ êµ¬í˜„
}

print("\n" + "="*80)
print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
print("="*80)

# %% [markdown]
# ## 7. í•™ìŠµ ê³¡ì„  ì‹œê°í™”

# %%
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Loss ê³¡ì„ 
axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)
axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Training and Validation Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# MSE ê³¡ì„ 
axes[0, 1].plot(history['train_mse'], label='Train MSE', linewidth=2, color='orange')
axes[0, 1].plot(history['val_mse'], label='Val MSE', linewidth=2, color='red')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MSE')
axes[0, 1].set_title('Training and Validation MSE')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# PSNR ê³¡ì„ 
axes[0, 2].plot(history['train_psnr'], label='Train PSNR', linewidth=2, color='purple')
axes[0, 2].plot(history['val_psnr'], label='Val PSNR', linewidth=2, color='magenta')
axes[0, 2].set_xlabel('Epoch')
axes[0, 2].set_ylabel('PSNR (dB)')
axes[0, 2].set_title('Training and Validation PSNR')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Learning rate ê³¡ì„ 
axes[1, 0].plot(history['learning_rate'], linewidth=2, color='green')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Learning Rate')
axes[1, 0].set_title('Learning Rate Schedule')
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].set_yscale('log')

# ìµœì¢… ë©”íŠ¸ë¦­ ìš”ì•½
axes[1, 1].axis('off')
summary_text = f"""
ğŸ“Š í•™ìŠµ ìµœì¢… ê²°ê³¼

Loss:
  â€¢ ìµœì¢… Train Loss: {history['train_loss'][-1]:.6f}
  â€¢ ìµœì¢… Val Loss: {history['val_loss'][-1]:.6f}
  â€¢ ìµœê³  Val Loss: {trainer.best_val_loss:.6f}

MSE:
  â€¢ ìµœì¢… Train MSE: {history['train_mse'][-1]:.6f}
  â€¢ ìµœì¢… Val MSE: {history['val_mse'][-1]:.6f}
  â€¢ ìµœê³  Val MSE: {min(history['val_mse']):.6f}

PSNR:
  â€¢ ìµœì¢… Train PSNR: {history['train_psnr'][-1]:.2f} dB
  â€¢ ìµœì¢… Val PSNR: {history['val_psnr'][-1]:.2f} dB
  â€¢ ìµœê³  Val PSNR: {max(history['val_psnr']):.2f} dB

í•™ìŠµë¥ :
  â€¢ ìµœì¢… LR: {history['learning_rate'][-1]:.2e}
"""
axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, family='monospace',
                verticalalignment='center')

# ë¹ˆ ê³µê°„
axes[1, 2].axis('off')

plt.tight_layout()
plt.show()

print("\nâœ… í•™ìŠµ ê³¡ì„  ì‹œê°í™” ì™„ë£Œ!")

# %% [markdown]
# ## 8. ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡ ìƒ˜í”Œ

# %%
print("\nğŸ” ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì˜ˆì¸¡ ìƒ˜í”Œ ìƒì„± ì¤‘...")

model.eval()
with torch.no_grad():
    # ê²€ì¦ ë°°ì¹˜ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    val_batch = next(iter(val_loader))
    inputs = val_batch['image'].to(device)
    targets = val_batch['target'].to(device)
    
    # ì˜ˆì¸¡
    predictions = model(inputs)
    
    # CPUë¡œ ì´ë™
    inputs = inputs.cpu().numpy()
    targets = targets.cpu().numpy()
    predictions = predictions.cpu().numpy()

# ì²˜ìŒ 4ê°œ ìƒ˜í”Œ ì‹œê°í™”
num_to_show = min(4, len(inputs))

fig, axes = plt.subplots(num_to_show, 4, figsize=(16, 4*num_to_show))
if num_to_show == 1:
    axes = axes.reshape(1, -1)

for idx in range(num_to_show):
    # ì…ë ¥
    axes[idx, 0].imshow(inputs[idx, 0], cmap='gray')
    axes[idx, 0].set_title(f'Sample {idx}: Input')
    axes[idx, 0].axis('off')
    
    # Ground Truth
    im1 = axes[idx, 1].imshow(targets[idx, 0], cmap='hsv', vmin=-np.pi, vmax=np.pi)
    axes[idx, 1].set_title(f'Sample {idx}: Ground Truth')
    axes[idx, 1].axis('off')
    plt.colorbar(im1, ax=axes[idx, 1], fraction=0.046)
    
    # Prediction
    im2 = axes[idx, 2].imshow(predictions[idx, 0], cmap='hsv', vmin=-np.pi, vmax=np.pi)
    axes[idx, 2].set_title(f'Sample {idx}: Prediction')
    axes[idx, 2].axis('off')
    plt.colorbar(im2, ax=axes[idx, 2], fraction=0.046)
    
    # Error map
    error = np.abs(targets[idx, 0] - predictions[idx, 0])
    im3 = axes[idx, 3].imshow(error, cmap='hot')
    axes[idx, 3].set_title(f'Sample {idx}: Error\nMAE={np.mean(error):.3f}')
    axes[idx, 3].axis('off')
    plt.colorbar(im3, ax=axes[idx, 3], fraction=0.046)

plt.tight_layout()
plt.show()

print(f"\nâœ… {num_to_show}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡ ì‹œê°í™” ì™„ë£Œ!")

# %% [markdown]
# ## 9. ë‹¤ìŒ ë‹¨ê³„
#
# ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”:
#
# **`04_sliding_window_prediction_notebook.py`**: ëŒ€í˜• ì´ë¯¸ì§€ ì˜ˆì¸¡

# %%

