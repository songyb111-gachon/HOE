# %% [markdown]
# # π“ Inverse Design U-Net λ¨λΈ ν•™μµ
#
# 256Γ—256 νƒ€μΌλ΅ Inverse Design U-Net λ¨λΈμ„ ν•™μµν•©λ‹λ‹¤.
#
# **λ¨λΈ:**
# - Input: Phase Map (λ©ν‘ μ„μƒ λ§µ)
# - Output: Pillar Pattern (κ·Έκ²ƒμ„ λ§λ“¤μ–΄λ‚Ό ν•„λ¬ ν¨ν„΄)
#
# ## π“‹ λ©μ°¨
# 1. ν™κ²½ μ„¤μ • λ° μ„ν¬νΈ
# 2. νλΌλ―Έν„° μ„¤μ •
# 3. λ°μ΄ν„° λ΅λ” μƒμ„±
# 4. λ¨λΈ μƒμ„±
# 5. ν•™μµ
# 6. κ²°κ³Ό μ‹κ°ν™”

# %% [markdown]
# ## 1. ν™κ²½ μ„¤μ • λ° μ„ν¬νΈ

# %%
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# PyTorch μ½”λ“ κ²½λ΅ μ¶”κ°€
sys.path.append('pytorch_codes')

from models import InverseDesignUNet
from datasets import InverseDesignDataset, create_dataloaders
from utils import WeightedBCELoss, Trainer

# GPU ν™•μΈ
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"β… PyTorch μ„¤μ • μ™„λ£!")
print(f"   Device: {device}")
print(f"   PyTorch λ²„μ „: {torch.__version__}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

# %% [markdown]
# ## 2. νλΌλ―Έν„° μ„¤μ •

# %%
# ==================== λ°μ΄ν„° νλΌλ―Έν„° ====================
DATA_PATH = 'data/inverse_tiles/train'
BATCH_SIZE = 16                    # νƒ€μΌ κΈ°λ°μ΄λ―€λ΅ λ” ν° λ°°μΉ μ‚¬μ© κ°€λ¥
NUM_WORKERS = 4                     # λ°μ΄ν„° λ΅λ”© μ›μ»¤

# ==================== λ¨λΈ νλΌλ―Έν„° ====================
LAYER_NUM = 5                       # U-Net λ μ΄μ–΄ μ
BASE_FEATURES = 64                  # κΈ°λ³Έ feature μ
DROPOUT_RATE = 0.2                  # Dropout λΉ„μ¨
USE_BATCHNORM = True                # BatchNorm μ‚¬μ© μ—¬λ¶€

# ==================== ν•™μµ νλΌλ―Έν„° ====================
NUM_EPOCHS = 100
LEARNING_RATE = 1e-4
LOSS_TYPE = 'weighted_bce'          # 'bce', 'weighted_bce'
PILLAR_WEIGHT = 2.0                 # Pillar ν΄λμ¤ κ°€μ¤‘μΉ (pillarκ°€ λ” μ¤‘μ”)

# ==================== μ²΄ν¬ν¬μΈνΈ νλΌλ―Έν„° ====================
CHECKPOINT_DIR = 'checkpoints'
LOG_DIR = 'logs'
EXPERIMENT_NAME = 'inverse_design_basic_tiles'
SAVE_FREQ = 5                       # N epochλ§λ‹¤ μ €μ¥

# λ””λ ‰ν† λ¦¬ μƒμ„±
Path(CHECKPOINT_DIR).mkdir(exist_ok=True)
Path(LOG_DIR).mkdir(exist_ok=True)

print("β… νλΌλ―Έν„° μ„¤μ • μ™„λ£!")
print(f"\nπ“ ν•™μµ μ„¤μ •:")
print(f"   λ°μ΄ν„° κ²½λ΅: {DATA_PATH}")
print(f"   λ°°μΉ ν¬κΈ°: {BATCH_SIZE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   ν•™μµλ¥ : {LEARNING_RATE}")
print(f"   μ†μ‹¤ ν•¨μ: {LOSS_TYPE}")
print(f"   Pillar κ°€μ¤‘μΉ: {PILLAR_WEIGHT}")
print(f"   Device: {device}")

# %% [markdown]
# ## 3. λ°μ΄ν„° λ΅λ” μƒμ„±

# %%
print("π“‚ λ°μ΄ν„° λ΅λ”© μ¤‘...")

# λ°μ΄ν„° λ΅λ” μƒμ„±
train_loader, val_loader, test_loader = create_dataloaders(
    dataset_path=DATA_PATH,
    dataset_type='inverse',
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    train_split=0.8,
    val_split=0.2,
    normalize=False
)

print("\nβ… λ°μ΄ν„° λ΅λ” μƒμ„± μ™„λ£!")
print(f"   ν›λ ¨ λ°°μΉ: {len(train_loader)}")
print(f"   κ²€μ¦ λ°°μΉ: {len(val_loader)}")

# μƒν” λ°μ΄ν„° ν™•μΈ
sample_batch = next(iter(train_loader))
print(f"\nπ“ μƒν” λ°°μΉ ν¬κΈ°:")
print(f"   Input (Phase Map): {sample_batch['image'].shape}")  # [B, 1, H, W]
print(f"   Target (Pillar): {sample_batch['target'].shape}")    # [B, 1, H, W]
print(f"   Input range: [{sample_batch['image'].min():.2f}, {sample_batch['image'].max():.2f}]")
print(f"   Target range: [{sample_batch['target'].min():.2f}, {sample_batch['target'].max():.2f}]")

# %% [markdown]
# ## 4. λ¨λΈ μƒμ„±

# %%
print("π”¨ λ¨λΈ μƒμ„± μ¤‘...")

# Inverse Design U-Net λ¨λΈ
model = InverseDesignUNet(
    in_channels=1,
    out_channels=1,
    layer_num=LAYER_NUM,
    base_features=BASE_FEATURES,
    dropout_rate=DROPOUT_RATE,
    use_batchnorm=USE_BATCHNORM
).to(device)

print(f"\nβ… λ¨λΈ μƒμ„± μ™„λ£!")
print(f"   λ¨λΈ: InverseDesignUNet")
print(f"   λ μ΄μ–΄ μ: {LAYER_NUM}")
print(f"   κΈ°λ³Έ features: {BASE_FEATURES}")
print(f"   Dropout: {DROPOUT_RATE}")
print(f"   BatchNorm: {USE_BATCHNORM}")

# λ¨λΈ νλΌλ―Έν„° μ κ³„μ‚°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nπ“ λ¨λΈ νλΌλ―Έν„°:")
print(f"   μ΄ νλΌλ―Έν„°: {total_params:,}")
print(f"   ν•™μµ κ°€λ¥ νλΌλ―Έν„°: {trainable_params:,}")

# %% [markdown]
# ## 5. μ†μ‹¤ ν•¨μ λ° μµν‹°λ§μ΄μ € μ„¤μ •

# %%
# μ†μ‹¤ ν•¨μ
if LOSS_TYPE == 'weighted_bce':
    criterion = WeightedBCELoss(pillar_weight=PILLAR_WEIGHT).to(device)
    print(f"β… μ†μ‹¤ ν•¨μ: Weighted BCE Loss (pillar_weight={PILLAR_WEIGHT})")
else:
    criterion = nn.BCEWithLogitsLoss().to(device)
    print(f"β… μ†μ‹¤ ν•¨μ: BCE Loss")

# μµν‹°λ§μ΄μ €
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
print(f"β… μµν‹°λ§μ΄μ €: Adam (lr={LEARNING_RATE})")

# Learning Rate Scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=10, 
    verbose=True
)
print(f"β… Scheduler: ReduceLROnPlateau (factor=0.5, patience=10)")

# %% [markdown]
# ## 6. ν•™μµ μ‹μ‘

# %%
print("\n" + "="*80)
print("π€ ν•™μµ μ‹μ‘!")
print("="*80)

# μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬
checkpoint_dir = Path(CHECKPOINT_DIR) / EXPERIMENT_NAME
checkpoint_dir.mkdir(parents=True, exist_ok=True)

# ν•™μµ νμ¤ν† λ¦¬
history = {
    'train_loss': [],
    'val_loss': [],
    'learning_rate': []
}

best_val_loss = float('inf')

# %% [markdown]
# ### ν•™μµ λ£¨ν”„

# %%
for epoch in range(NUM_EPOCHS):
    print(f"\n{'='*80}")
    print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
    print(f"{'='*80}")
    
    # ==================== ν›λ ¨ ====================
    model.train()
    train_loss = 0.0
    
    for batch_idx, batch in enumerate(train_loader):
        # λ°μ΄ν„° μ΄λ™
        inputs = batch['image'].to(device)      # Phase map
        targets = batch['target'].to(device)    # Pillar pattern
        
        # Forward
        optimizer.zero_grad()
        outputs = model(inputs)
        
        # Loss
        loss = criterion(outputs, targets)
        
        # Backward
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        
        # μ§„ν–‰μƒν™© μ¶λ ¥
        if (batch_idx + 1) % 50 == 0:
            print(f"  [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}")
    
    avg_train_loss = train_loss / len(train_loader)
    history['train_loss'].append(avg_train_loss)
    
    # ==================== κ²€μ¦ ====================
    model.eval()
    val_loss = 0.0
    
    with torch.no_grad():
        for batch in val_loader:
            inputs = batch['image'].to(device)
            targets = batch['target'].to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    history['val_loss'].append(avg_val_loss)
    history['learning_rate'].append(optimizer.param_groups[0]['lr'])
    
    # Learning rate scheduler
    scheduler.step(avg_val_loss)
    
    # κ²°κ³Ό μ¶λ ¥
    print(f"\n  π“ Epoch {epoch+1} κ²°κ³Ό:")
    print(f"     Train Loss: {avg_train_loss:.6f}")
    print(f"     Val Loss:   {avg_val_loss:.6f}")
    print(f"     LR: {optimizer.param_groups[0]['lr']:.6f}")
    
    # μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': avg_val_loss,
            'train_loss': avg_train_loss,
        }, checkpoint_dir / 'best_model.pth')
        print(f"     β… μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥! (Val Loss: {avg_val_loss:.6f})")
    
    # μ£ΌκΈ°μ  μ²΄ν¬ν¬μΈνΈ μ €μ¥
    if (epoch + 1) % SAVE_FREQ == 0:
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': avg_val_loss,
            'train_loss': avg_train_loss,
        }, checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth')
        print(f"     π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥: epoch_{epoch+1}")

print("\n" + "="*80)
print("β… ν•™μµ μ™„λ£!")
print("="*80)
print(f"   μµκ³  κ²€μ¦ μ†μ‹¤: {best_val_loss:.6f}")
print(f"   λ¨λΈ μ €μ¥ μ„μΉ: {checkpoint_dir}")

# %% [markdown]
# ## 7. ν•™μµ κ³΅μ„  μ‹κ°ν™”

# %%
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Loss
axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)
axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training and Validation Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Learning Rate
axes[1].plot(history['learning_rate'], linewidth=2, color='orange')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Learning Rate')
axes[1].set_title('Learning Rate Schedule')
axes[1].set_yscale('log')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(checkpoint_dir / 'training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"\nβ… ν•™μµ κ³΅μ„  μ €μ¥: {checkpoint_dir / 'training_curves.png'}")

# %% [markdown]
# ## 8. κ²€μ¦ μ„ΈνΈμ—μ„ μμΈ΅ μ‹κ°ν™”

# %%
print("\n" + "="*80)
print("π“ κ²€μ¦ μ„ΈνΈ μμΈ΅ μ‹κ°ν™”")
print("="*80)

# μµκ³  μ„±λ¥ λ¨λΈ λ΅λ“
checkpoint = torch.load(checkpoint_dir / 'best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# κ²€μ¦ μƒν” κ°€μ Έμ¤κΈ°
val_samples = next(iter(val_loader))
inputs = val_samples['image'].to(device)
targets = val_samples['target'].to(device)

with torch.no_grad():
    outputs = model(inputs)
    predictions = torch.sigmoid(outputs)  # [0, 1] ν™•λ¥ λ΅ λ³€ν™
    binary_predictions = (predictions > 0.5).float()  # μ΄μ§„ν™”

# μ‹κ°ν™” (4κ° μƒν”)
num_samples = min(4, inputs.shape[0])
fig, axes = plt.subplots(num_samples, 4, figsize=(16, num_samples*4))

if num_samples == 1:
    axes = axes.reshape(1, -1)

for i in range(num_samples):
    # Input: Phase map
    input_img = inputs[i, 0].cpu().numpy()
    axes[i, 0].imshow(input_img, cmap='twilight')
    axes[i, 0].set_title(f'Input: Phase Map\nRange: [{input_img.min():.2f}, {input_img.max():.2f}]')
    axes[i, 0].axis('off')
    
    # Target: Pillar pattern
    target_img = targets[i, 0].cpu().numpy()
    axes[i, 1].imshow(target_img, cmap='gray')
    axes[i, 1].set_title(f'Target: Pillar Pattern\nRange: [{target_img.min():.2f}, {target_img.max():.2f}]')
    axes[i, 1].axis('off')
    
    # Prediction: Probability map
    pred_prob = predictions[i, 0].cpu().numpy()
    axes[i, 2].imshow(pred_prob, cmap='gray', vmin=0, vmax=1)
    axes[i, 2].set_title(f'Prediction: Probability\nRange: [{pred_prob.min():.2f}, {pred_prob.max():.2f}]')
    axes[i, 2].axis('off')
    
    # Prediction: Binary
    pred_binary = binary_predictions[i, 0].cpu().numpy()
    axes[i, 3].imshow(pred_binary, cmap='gray')
    axes[i, 3].set_title(f'Prediction: Binary (>0.5)\nPillar ratio: {pred_binary.mean():.2%}')
    axes[i, 3].axis('off')

plt.tight_layout()
plt.savefig(checkpoint_dir / 'validation_predictions.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"β… κ²€μ¦ μμΈ΅ μ €μ¥: {checkpoint_dir / 'validation_predictions.png'}")

# %% [markdown]
# ## 9. μ™„λ£!
#
# Inverse Design λ¨λΈ ν•™μµμ΄ μ™„λ£λμ—μµλ‹λ‹¤! π‰
#
# **λ‹¤μ λ‹¨κ³„:**
# - `07_inverse_design_notebook.py`: μ›ν•λ” phase mapμΌλ΅λ¶€ν„° pillar pattern μ„¤κ³„

# %%
print("\n" + "="*80)
print("π‰ Inverse Design λ¨λΈ ν•™μµ μ™„λ£!")
print("="*80)
print(f"\nπ“‚ μ €μ¥λ νμΌ:")
print(f"   {checkpoint_dir / 'best_model.pth'}")
print(f"   {checkpoint_dir / 'training_curves.png'}")
print(f"   {checkpoint_dir / 'validation_predictions.png'}")
print(f"\nπ€ λ‹¤μ λ‹¨κ³„: 07_inverse_design_notebook.py μ‹¤ν–‰!")

