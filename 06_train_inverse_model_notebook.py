# %% [markdown]
# # ğŸ“ Inverse Design U-Net ëª¨ë¸ í•™ìŠµ
#
# 256Ã—256 íƒ€ì¼ë¡œ Inverse Design U-Net ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
#
# **ëª¨ë¸:**
# - Input: EM Intensity Map (ëª©í‘œ ìœ„ìƒ ë§µ)
# - Output: Pillar Pattern (ê·¸ê²ƒì„ ë§Œë“¤ì–´ë‚¼ í•„ëŸ¬ íŒ¨í„´)
#
# ## ğŸ“‹ ëª©ì°¨
# 1. í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸
# 2. íŒŒë¼ë¯¸í„° ì„¤ì •
# 3. ë°ì´í„° ë¡œë” ìƒì„±
# 4. ëª¨ë¸ ìƒì„±
# 5. í•™ìŠµ
# 6. ê²°ê³¼ ì‹œê°í™”

# %% [markdown]
# ## 1. í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸

# %%
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# PyTorch ì½”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append('pytorch_codes')

from models import InverseUNet
from datasets import InverseDesignDataset, create_dataloaders
from utils import Trainer

# GPU í™•ì¸
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"âœ… PyTorch ì„¤ì • ì™„ë£Œ!")
print(f"   Device: {device}")
print(f"   PyTorch ë²„ì „: {torch.__version__}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

# %% [markdown]
# ## 2. íŒŒë¼ë¯¸í„° ì„¤ì •

# %%
# ==================== ë°ì´í„° íŒŒë¼ë¯¸í„° ====================
DATA_PATH = 'data/inverse_tiles/train'
BATCH_SIZE = 16                    # íƒ€ì¼ ê¸°ë°˜ì´ë¯€ë¡œ ë” í° ë°°ì¹˜ ì‚¬ìš© ê°€ëŠ¥
NUM_WORKERS = 4                     # ë°ì´í„° ë¡œë”© ì›Œì»¤

# ==================== ëª¨ë¸ íŒŒë¼ë¯¸í„° ====================
LAYER_NUM = 5                       # U-Net ë ˆì´ì–´ ìˆ˜
BASE_FEATURES = 64                  # ê¸°ë³¸ feature ìˆ˜
DROPOUT_RATE = 0.2                  # Dropout ë¹„ìœ¨
USE_BATCHNORM = True                # BatchNorm ì‚¬ìš© ì—¬ë¶€

# ==================== í•™ìŠµ íŒŒë¼ë¯¸í„° ====================
NUM_EPOCHS = 100
LEARNING_RATE = 1e-4
LOSS_TYPE = 'weighted_bce'          # 'bce', 'weighted_bce'
PILLAR_WEIGHT = 2.0                 # Pillar í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (pillarê°€ ë” ì¤‘ìš”)

# ==================== ì²´í¬í¬ì¸íŠ¸ íŒŒë¼ë¯¸í„° ====================
CHECKPOINT_DIR = 'checkpoints'
LOG_DIR = 'logs'
EXPERIMENT_NAME = 'inverse_design_basic_tiles'
SAVE_FREQ = 5                       # N epochë§ˆë‹¤ ì €ì¥

# ë””ë ‰í† ë¦¬ ìƒì„±
Path(CHECKPOINT_DIR).mkdir(exist_ok=True)
Path(LOG_DIR).mkdir(exist_ok=True)

print("âœ… íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!")
print(f"\nğŸ“Š í•™ìŠµ ì„¤ì •:")
print(f"   ë°ì´í„° ê²½ë¡œ: {DATA_PATH}")
print(f"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   í•™ìŠµë¥ : {LEARNING_RATE}")
print(f"   ì†ì‹¤ í•¨ìˆ˜: {LOSS_TYPE}")
print(f"   Pillar ê°€ì¤‘ì¹˜: {PILLAR_WEIGHT}")
print(f"   Device: {device}")

# %% [markdown]
# ## 3. ë°ì´í„° ë¡œë” ìƒì„±

# %%
print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

from torch.utils.data import DataLoader

# ë°ì´í„°ì…‹ ìƒì„± (ì´ë¯¸ train/valë¡œ ë‚˜ë‰˜ì–´ì ¸ ìˆìŒ)
train_dataset = InverseDesignDataset(
    data_path='data/inverse_tiles/train',
    input_extension='npy',
    output_extension='png',
    normalize=False
)

val_dataset = InverseDesignDataset(
    data_path='data/inverse_tiles/val',
    input_extension='npy',
    output_extension='png',
    normalize=False
)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS
)

print("\nâœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!")
print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset)} ({len(train_loader)} ë°°ì¹˜)")
print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)} ({len(val_loader)} ë°°ì¹˜)")

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
sample_batch = next(iter(train_loader))
print(f"\nğŸ“Š ìƒ˜í”Œ ë°°ì¹˜ í¬ê¸°:")
print(f"   Input (EM Intensity Map): {sample_batch['image'].shape}")  # [B, 1, H, W]
print(f"   Target (Pillar): {sample_batch['target'].shape}")    # [B, 1, H, W]
print(f"   Input range: [{sample_batch['image'].min():.2f}, {sample_batch['image'].max():.2f}]")
print(f"   Target range: [{sample_batch['target'].min():.2f}, {sample_batch['target'].max():.2f}]")

# %% [markdown]
# ## 4. ëª¨ë¸ ìƒì„±

# %%
print("ğŸ”¨ ëª¨ë¸ ìƒì„± ì¤‘...")

# Inverse Design U-Net ëª¨ë¸
model = InverseUNet(
    in_channels=1,
    out_channels=[1],
    layer_num=LAYER_NUM,
    base_features=BASE_FEATURES,
    dropout_rate=DROPOUT_RATE,
    output_activations=['linear'],  # BCEWithLogitsLossë¥¼ ìœ„í•´ linear ì‚¬ìš©
    use_batchnorm=USE_BATCHNORM
).to(device)

print(f"\nâœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
print(f"   ëª¨ë¸: InverseUNet")
print(f"   ë ˆì´ì–´ ìˆ˜: {LAYER_NUM}")
print(f"   ê¸°ë³¸ features: {BASE_FEATURES}")
print(f"   Dropout: {DROPOUT_RATE}")
print(f"   BatchNorm: {USE_BATCHNORM}")

# ëª¨ë¸ ìš”ì•½ ì¶œë ¥
model.get_model_summary()

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
print(f"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

# %% [markdown]
# ## 5. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •

# %%
# ì†ì‹¤ í•¨ìˆ˜
if LOSS_TYPE == 'weighted_bce':
    # pos_weight: pillar í´ë˜ìŠ¤(1)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    pos_weight = torch.tensor([PILLAR_WEIGHT]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"âœ… ì†ì‹¤ í•¨ìˆ˜: Weighted BCE Loss (pillar_weight={PILLAR_WEIGHT})")
else:
    criterion = nn.BCEWithLogitsLoss()
    print(f"âœ… ì†ì‹¤ í•¨ìˆ˜: BCE Loss")

# ì˜µí‹°ë§ˆì´ì €
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
print(f"âœ… ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE})")

# Learning Rate Scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=10, 
    verbose=True
)
print(f"âœ… Scheduler: ReduceLROnPlateau (factor=0.5, patience=10)")

# %% [markdown]
# ## 6. í•™ìŠµ ì‹œì‘

# %%
print("\n" + "="*80)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*80)

# Trainerë¥¼ ì‚¬ìš©í•œ í•™ìŠµ
trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    checkpoint_dir=CHECKPOINT_DIR,
    log_dir=LOG_DIR,
    experiment_name=EXPERIMENT_NAME
)

# í•™ìŠµ ì‹¤í–‰
trainer.train(
    num_epochs=NUM_EPOCHS,
    save_freq=SAVE_FREQ
)

# í•™ìŠµ íˆìŠ¤í† ë¦¬ êµ¬ì„±
history = {
    'train_loss': trainer.train_losses,
    'val_loss': trainer.val_losses,
    'train_mse': trainer.train_mse,
    'val_mse': trainer.val_mse,
    'train_psnr': trainer.train_psnr,
    'val_psnr': trainer.val_psnr,
    'learning_rate': [optimizer.param_groups[0]['lr']] * NUM_EPOCHS
}

print("\n" + "="*80)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print("="*80)
print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {trainer.best_val_loss:.6f}")
print(f"   ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {trainer.checkpoint_dir}")

# %% [markdown]
# ## 7. í•™ìŠµ ê³¡ì„  ì‹œê°í™”

# %%
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Loss (BCE)
axes[0, 0].plot(history['train_loss'], label='Train Loss (BCE)', linewidth=2)
axes[0, 0].plot(history['val_loss'], label='Val Loss (BCE)', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Training and Validation Loss (BCE)')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# MSE
axes[0, 1].plot(history['train_mse'], label='Train MSE', linewidth=2, color='orange')
axes[0, 1].plot(history['val_mse'], label='Val MSE', linewidth=2, color='red')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MSE')
axes[0, 1].set_title('Training and Validation MSE')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# PSNR
axes[0, 2].plot(history['train_psnr'], label='Train PSNR', linewidth=2, color='purple')
axes[0, 2].plot(history['val_psnr'], label='Val PSNR', linewidth=2, color='magenta')
axes[0, 2].set_xlabel('Epoch')
axes[0, 2].set_ylabel('PSNR (dB)')
axes[0, 2].set_title('Training and Validation PSNR')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Learning Rate
axes[1, 0].plot(history['learning_rate'], linewidth=2, color='green')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Learning Rate')
axes[1, 0].set_title('Learning Rate Schedule')
axes[1, 0].set_yscale('log')
axes[1, 0].grid(True, alpha=0.3)

# ìµœì¢… ë©”íŠ¸ë¦­ ìš”ì•½
axes[1, 1].axis('off')
summary_text = f"""
ğŸ“Š í•™ìŠµ ìµœì¢… ê²°ê³¼

BCE Loss:
  â€¢ ìµœì¢… Train Loss: {history['train_loss'][-1]:.6f}
  â€¢ ìµœì¢… Val Loss: {history['val_loss'][-1]:.6f}
  â€¢ ìµœê³  Val Loss: {trainer.best_val_loss:.6f}

MSE:
  â€¢ ìµœì¢… Train MSE: {history['train_mse'][-1]:.6f}
  â€¢ ìµœì¢… Val MSE: {history['val_mse'][-1]:.6f}
  â€¢ ìµœê³  Val MSE: {min(history['val_mse']):.6f}

PSNR:
  â€¢ ìµœì¢… Train PSNR: {history['train_psnr'][-1]:.2f} dB
  â€¢ ìµœì¢… Val PSNR: {history['val_psnr'][-1]:.2f} dB
  â€¢ ìµœê³  Val PSNR: {max(history['val_psnr']):.2f} dB

í•™ìŠµë¥ :
  â€¢ ìµœì¢… LR: {history['learning_rate'][-1]:.2e}
"""
axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, family='monospace',
                verticalalignment='center')

# ë¹ˆ ê³µê°„
axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig(trainer.checkpoint_dir / 'training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"\nâœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {trainer.checkpoint_dir / 'training_curves.png'}")

# %% [markdown]
# ## 8. ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì˜ˆì¸¡ ì‹œê°í™”

# %%
print("\n" + "="*80)
print("ğŸ“Š ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡ ì‹œê°í™”")
print("="*80)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
checkpoint = torch.load(trainer.checkpoint_dir / 'best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ê²€ì¦ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
val_samples = next(iter(val_loader))
inputs = val_samples['image'].to(device)
targets = val_samples['target'].to(device)

with torch.no_grad():
    outputs = model(inputs)
    predictions = torch.sigmoid(outputs)  # [0, 1] í™•ë¥ ë¡œ ë³€í™˜
    binary_predictions = (predictions > 0.5).float()  # ì´ì§„í™”

# ì‹œê°í™” (4ê°œ ìƒ˜í”Œ)
num_samples = min(4, inputs.shape[0])
fig, axes = plt.subplots(num_samples, 4, figsize=(16, num_samples*4))

if num_samples == 1:
    axes = axes.reshape(1, -1)

for i in range(num_samples):
    # Input: Phase map
    input_img = inputs[i, 0].cpu().numpy()
    axes[i, 0].imshow(input_img, cmap='twilight')
    axes[i, 0].set_title(f'Input: EM Intensity Map\nRange: [{input_img.min():.2f}, {input_img.max():.2f}]')
    axes[i, 0].axis('off')
    
    # Target: Pillar pattern
    target_img = targets[i, 0].cpu().numpy()
    axes[i, 1].imshow(target_img, cmap='gray')
    axes[i, 1].set_title(f'Target: Pillar Pattern\nRange: [{target_img.min():.2f}, {target_img.max():.2f}]')
    axes[i, 1].axis('off')
    
    # Prediction: Probability map
    pred_prob = predictions[i, 0].cpu().numpy()
    axes[i, 2].imshow(pred_prob, cmap='gray', vmin=0, vmax=1)
    axes[i, 2].set_title(f'Prediction: Probability\nRange: [{pred_prob.min():.2f}, {pred_prob.max():.2f}]')
    axes[i, 2].axis('off')
    
    # Prediction: Binary
    pred_binary = binary_predictions[i, 0].cpu().numpy()
    axes[i, 3].imshow(pred_binary, cmap='gray')
    axes[i, 3].set_title(f'Prediction: Binary (>0.5)\nPillar ratio: {pred_binary.mean():.2%}')
    axes[i, 3].axis('off')

plt.tight_layout()
plt.savefig(trainer.checkpoint_dir / 'validation_predictions.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"âœ… ê²€ì¦ ì˜ˆì¸¡ ì €ì¥: {trainer.checkpoint_dir / 'validation_predictions.png'}")

# %% [markdown]
# ## 9. ì™„ë£Œ!
#
# Inverse Design ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰
#
# **ë‹¤ìŒ ë‹¨ê³„:**
# - `07_inverse_design_notebook.py`: ì›í•˜ëŠ” phase mapìœ¼ë¡œë¶€í„° pillar pattern ì„¤ê³„

# %%
print("\n" + "="*80)
print("ğŸ‰ Inverse Design ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("="*80)
print(f"\nğŸ“‚ ì €ì¥ëœ íŒŒì¼:")
print(f"   {trainer.checkpoint_dir / 'best_model.pth'}")
print(f"   {trainer.checkpoint_dir / 'training_curves.png'}")
print(f"   {trainer.checkpoint_dir / 'validation_predictions.png'}")
print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: 07_inverse_design_notebook.py ì‹¤í–‰!")

